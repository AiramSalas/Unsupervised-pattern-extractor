{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280603af-ad6a-449c-9bb6-b2bdd57c5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1838d01f-98bc-4653-b1e5-313b8cc0c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths by adding ../src (where paths.py is located) to sys.path\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "SRC_DIR = NOTEBOOK_DIR.parent / \"src\"\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# Now import the shared paths\n",
    "from paths import CSV_DIR, DATAFRAMES_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8228d-126e-4083-96fa-6db0c5ba4473",
   "metadata": {},
   "source": [
    "### First load the info about the dates of the original experiment.\n",
    "#### Each observatory ('Site' in the Dfs) will have it's own dataframe. \n",
    "##### OT => Observatory Teide\n",
    "##### ORM => Obervatory Roque de los Muchachos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c670be-5f53-4187-bfd9-7b10815f6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teide_dates = pd.read_csv(CSV_DIR / \"OT_dates.csv\")\n",
    "df_roque_dates = pd.read_csv(CSV_DIR / \"ORM_dates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae34868-83df-4a19-aeb6-75020fc21867",
   "metadata": {},
   "source": [
    "### Then the meteo info provided, so we can filter them to contain only the data about the same days than the turbulence measurements. Meteo info is previously filtered when processing it from original .dat files, grouped by date and hour then calculate the mean() of each variable ('Temperature', 'Humidity','Wind_dir', 'Wind_speed', 'Pressure', 'Solar_radiation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0d0ba4-bbf3-4513-9a26-e8eac669afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teide_meteo = pd.read_parquet(DATAFRAMES_DIR / \"df_teide_meteo.parquet\")\n",
    "df_roque_meteo = pd.read_parquet(DATAFRAMES_DIR / \"df_roque_meteo.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34921bd-e7ef-468d-a363-14d4f425ace0",
   "metadata": {},
   "source": [
    "### Loading turbulence DFs grouped by profile_id = each set of 79 measurements of altitude/turbulence with same metadata, Site, Star,.... They are parsed from the original .dat files and some time related columns were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35dea02-7166-45d0-b9d6-e16ff7e6f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teide = pd.read_parquet(DATAFRAMES_DIR / \"df_teide_pid.parquet\")\n",
    "df_roque = pd.read_parquet(DATAFRAMES_DIR / \"df_roque_pid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3be5be-ab7b-4852-8ae6-c93f08e1142c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Site', 'Star', 'cplane', 'sep', 'Deltah0', 'Res', 'day', 'month',\n",
       "       'year', 'hbegin', 'hend', 'Seeing_totatm', 'bl', 'fa', 'altitude',\n",
       "       'turbulence', 'profile_id', 'date_base', 'timestamp_begin',\n",
       "       'timestamp_end', 'seconds_since_midnight_begin',\n",
       "       'seconds_since_midnight_end', 'sin_time_begin', 'cos_time_begin',\n",
       "       'sin_time_end', 'cos_time_end', 'duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teide.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db2d7f7c-f1a4-4de8-b5ab-4854ca79e07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'Month', 'Day', 'hour', 'Temperature', 'Humidity', 'Wind_dir',\n",
       "       'Wind_speed', 'Pressure', 'Solar_radiation', 'date_base'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teide_meteo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5147c6c-e5b3-41d1-9904-cbdc8036aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_dates(df_dates,df_meteo):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_dates : DataFrame, df with original dates, from which a new one will be generated with extended dates.\n",
    "    df_meteo : DataFrame with meteorological information, useful to know from which dates we have weather info\n",
    "    \"\"\"\n",
    "    #Lets filter meteo with dates +/- 1 day using date_base column of dates dfs, to cover all the posibilities with hbegin and hend in the turbulence DF\n",
    "    \n",
    "    #Convert date_base to dateTime object and create the list of available dates\n",
    "    df_dates['date_base'] = pd.to_datetime(df_dates['date_base'])\n",
    "    \n",
    "    #I have checked duplicated dates previously , but just to have them double checked and in array format \n",
    "    original_dates = df_dates['date_base'].unique()\n",
    "    \n",
    "    #Create new set with date -1 and +1\n",
    "    extended_dates = set(original_dates)\n",
    "    for d in original_dates:\n",
    "        extended_dates.add(d - pd.Timedelta(days=1))#previous day\n",
    "        extended_dates.add(d + pd.Timedelta(days=1))#following day\n",
    "    \n",
    "    # Remove any duplicates and keep only new dates\n",
    "    all_dates = pd.Series(sorted(extended_dates))\n",
    "    #filters all_dates to keep only the ones not in the original set. ~ NOT â€” it inverts the booleans \n",
    "    missing_dates = all_dates[~all_dates.isin(original_dates)]\n",
    "\n",
    "    #Create a new DataFrame with missing dates, \n",
    "    #missing  = the ones 1 day after or 1 day before the ones previously present\n",
    "    df_extra_dates = pd.DataFrame({'date_base': missing_dates})\n",
    "    \n",
    "    # Concatenate original dates + extra dates generated\n",
    "    df_extended_dates = pd.concat([df_dates, df_extra_dates], ignore_index=True)\n",
    "    \n",
    "    # Sort by date, just for better visualization\n",
    "    df_extended_dates = df_extended_dates.sort_values(by='date_base').reset_index(drop=True)\n",
    "\n",
    "    #Rows from meteo DFs, which has more available dates than measurements dates, where date_base is included in the extended list, \n",
    "    #so it should be a 'possible match' with measurements DF\n",
    "    #This is mainly the possible variances when calculating real day based on hbegin and hend, \n",
    "    #since they are fractional hours respect to midnight of the day when the measurement began\n",
    "    df_meteo_filtered_dates = df_meteo[df_meteo['date_base'].isin(df_extended_dates['date_base'])].copy()\n",
    "\n",
    "    #Add in meteo df a timestamp_begin just to be able to merge by timestamp_begin and hour, it will be YYYY-MM-DD 00:00:00\n",
    "    df_meteo_filtered_dates.loc[:,'timestamp_begin_date'] = df_meteo_filtered_dates['date_base'].dt.date\n",
    "    #convert it to datetime object\n",
    "    df_meteo_filtered_dates['timestamp_begin_date'] = pd.to_datetime(df_meteo_filtered_dates['timestamp_begin_date'])\n",
    "\n",
    "    #return df_extended_dates\n",
    "    return df_meteo_filtered_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e22105d-266b-472f-8323-d68425001c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teide_meteo_extended = extend_dates(df_teide_dates,df_teide_meteo)\n",
    "df_roque_meteo_extended = extend_dates(df_roque_dates,df_roque_meteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b13fef7-6fc0-4bf4-884a-1779f959836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_data_to_measurement_df(df):\n",
    "    #Add some time related columns to main measurements DF just to facilitate the merge process between DF's\n",
    "    #So we will use date and hour to match measurements and meteo data\n",
    "    df['hour'] = df['timestamp_begin'].dt.hour\n",
    "    #Create a new column with only date from timestamp, not hours,min, or secs \n",
    "    df.loc[:,'timestamp_begin_date'] = df['timestamp_begin'].dt.date\n",
    "    #Setting as datetime, not object\n",
    "    df['timestamp_begin_date'] = pd.to_datetime(df['timestamp_begin_date'])\n",
    "    \n",
    "    print(f\"Extra time info added to DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eb5ae45-6305-49ca-9593-8ea53a8faa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra time info added to DataFrame\n",
      "Extra time info added to DataFrame\n"
     ]
    }
   ],
   "source": [
    "add_time_data_to_measurement_df(df_teide)\n",
    "add_time_data_to_measurement_df(df_roque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f86bec-3f66-4dc3-9575-c29530a3c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove redundant columns, generated when extending dates\n",
    "df_teide_meteo_extended = df_teide_meteo_extended.drop(columns=['date_base'])\n",
    "df_roque_meteo_extended = df_roque_meteo_extended.drop(columns=['date_base'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838e728-f961-4dd4-8994-e86363dd785c",
   "metadata": {},
   "source": [
    "### Merge meteo filtered data with turbulence DFs , base on timestamp begin then hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17acbe84-c7dd-4f30-8f31-1c803f816b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teide_merged = df_teide.merge(df_teide_meteo_extended, on=['timestamp_begin_date', 'hour'], how='left')\n",
    "df_roque_merged = df_roque.merge(df_roque_meteo_extended, on=['timestamp_begin_date', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8f14a-d5c8-4ba1-9f64-c08e27f47da4",
   "metadata": {},
   "source": [
    "#### Once merge it's done , I add an extra turbulence_log column to be used for visualizations mainly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4733b41-56ad-4259-abd2-f7c854077975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimum values without zero values, since there are some rows with 0 value in turbulence as consequence of wrong measurements\n",
    "turbulence_min_teide = df_teide_merged[df_teide_merged[\"turbulence\"] > 0.0][\"turbulence\"].min()\n",
    "turbulence_min_roque = df_roque_merged[df_roque_merged[\"turbulence\"] > 0.0][\"turbulence\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a362342c-cb95-4a6d-9b29-48a419cec3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a log tranformation for turbulence values since they are really tiny, \n",
    "# this should improve visualization and model performance\n",
    "# Use the actual minimum value as epsilon to avoid log(0)\n",
    "def turbulence_log(df, turbulence_min):\n",
    "    epsilon = turbulence_min\n",
    "    # Scale the values by a large factor then apply log to avoid rounding or truncation issues \n",
    "    scaling_factor = 1e20\n",
    "    \n",
    "    #Apply log transformation\n",
    "    df['turbulence_log'] = df['turbulence'].apply(\n",
    "        lambda val: np.log(val * scaling_factor + epsilon) if val > 0 else np.nan\n",
    "    ).copy()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3179d89-46f3-4dc2-9d7b-6250f5ce2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column turbulence log to Main dfs \n",
    "df_teide_merged = turbulence_log(df_teide_merged,turbulence_min_teide)\n",
    "df_roque_merged = turbulence_log(df_roque_merged,turbulence_min_roque)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432c6b38-93e2-4ebe-bcd1-7aa95d75527a",
   "metadata": {},
   "source": [
    "### The resulting DFs will include all related data. \n",
    "#### Metadata\n",
    "#### Dates, hbegin and hend windows\n",
    "#### Altitude + turbulence pairs \n",
    "#### Some extra time related features , duration , sin and cos\n",
    "#### Meteo data averaged by hours of the same day that matches hours when the measurement was taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b478a7cf-cb66-4a5a-986b-1e9242f44686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_merged_df(df_merged):\n",
    "    print(f\" ðŸª£ ðŸ§½  Cleaning the DataFrame...\")\n",
    "    #Drop NaN produced due to meteo data not present in the same hours than the turbulence measurements.\n",
    "    #So the resulting df has values in all of its columns.\n",
    "    df_merged = df_merged.dropna()\n",
    "    #Remove some more redundant columns after merge\n",
    "    df_merged = df_merged.drop(['day', 'month', 'year', 'date_base'],axis=1)\n",
    "    #Modify some datatypes to improve size and performance when operating over the complete df \n",
    "    df_merged['cplane'] = df_merged['cplane'].astype('float32')\n",
    "    df_merged['sep'] = df_merged['sep'].astype('float32')\n",
    "    df_merged['Deltah0'] = df_merged['Deltah0'].astype('int16')\n",
    "    df_merged['hbegin'] = df_merged['hbegin'].astype('float64')\n",
    "    df_merged['hend'] = df_merged['hend'].astype('float64')\n",
    "    df_merged['Seeing_totatm'] = df_merged['Seeing_totatm'].astype('float32')\n",
    "    df_merged['bl'] = df_merged['bl'].astype('float32')\n",
    "    df_merged['fa'] = df_merged['fa'].astype('float32')\n",
    "    df_merged['profile_id'] = df_merged['profile_id'].astype('int32')\n",
    "    df_merged['seconds_since_midnight_begin'] = df_merged['seconds_since_midnight_begin'].astype('float64')\n",
    "    df_merged['seconds_since_midnight_end'] = df_merged['seconds_since_midnight_end'].astype('float64')\n",
    "    df_merged['duration'] = df_merged['duration'].astype('float64')\n",
    "    #Columns from meteo DF's after .dat files parse\n",
    "    df_merged['hour'] = df_merged['hour'].astype('int8')\n",
    "    df_merged['Year'] = df_merged['Year'].astype('int16')\n",
    "    df_merged['Month'] = df_merged['Month'].astype('int8')\n",
    "    df_merged['Day'] = df_merged['Day'].astype('int8')\n",
    "    print(f\"ðŸ§¹ DataFrame clean and ready to use...\")\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05f1cd53-c5b8-453e-a4b3-0f0077979a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸª£ ðŸ§½  Cleaning the DataFrame...\n",
      "ðŸ§¹ DataFrame clean and ready to use...\n"
     ]
    }
   ],
   "source": [
    "df_roque_cleaned = clean_merged_df(df_roque_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "259a0b9c-1d26-42c3-b67e-f34f1428c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ðŸª£ ðŸ§½  Cleaning the DataFrame...\n",
      "ðŸ§¹ DataFrame clean and ready to use...\n"
     ]
    }
   ],
   "source": [
    "df_teide_cleaned = clean_merged_df(df_teide_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "801f36d1-1fa2-4645-83f8-d20eb21b275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save both DataFrames as .parquet files, after finishing all processing\n",
    "df_teide_cleaned.to_parquet(DATAFRAMES_DIR / \"df_teide_full_Info.parquet\", index=False, engine='pyarrow')\n",
    "df_roque_cleaned.to_parquet(DATAFRAMES_DIR / \"df_roque_full_Info.parquet\", index=False, engine='pyarrow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
